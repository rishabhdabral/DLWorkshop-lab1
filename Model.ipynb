{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.10"},"colab":{"name":"Model.ipynb","provenance":[],"toc_visible":true}},"cells":[{"cell_type":"code","metadata":{"id":"6h--9X_u19rd","colab_type":"code","colab":{}},"source":["import numpy as np\n","import torch\n","import torch.nn as nn"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"pjqclHrk19rp","colab_type":"text"},"source":["# The Simplest Neural Net\n","\n","Let's try to implement a simple, one hidden-layered fully connected neural network using torch.nn package. For our example, we consider a 10-dimensional input and 2-dimensional output with the hidden layer of 50 neurons."]},{"cell_type":"code","metadata":{"id":"BZq_FqAz19rv","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":173},"outputId":"e7a75c25-d754-4d57-9b93-3e498774ed70","executionInfo":{"status":"ok","timestamp":1583096600849,"user_tz":-330,"elapsed":1833,"user":{"displayName":"Rahul Bishain","photoUrl":"","userId":"06667514033618368895"}}},"source":["input_dim = 10\n","output_dim = 2\n","hidden_dim = 50\n","\n","# Sequential is a container that can hold sequentially occurring layers\n","model = nn.Sequential(\n","            nn.Linear(input_dim, hidden_dim),\n","            nn.ReLU(),\n","            nn.Linear(hidden_dim, output_dim))\n","\n","# Let's try to print the model and see what turns up\n","print(model)\n","\n","# Let's also see the 'learnable' network parameters. This will be used later\n","for i, param  in enumerate(model.parameters()):\n","    print(param.shape)"],"execution_count":2,"outputs":[{"output_type":"stream","text":["Sequential(\n","  (0): Linear(in_features=10, out_features=50, bias=True)\n","  (1): ReLU()\n","  (2): Linear(in_features=50, out_features=2, bias=True)\n",")\n","torch.Size([50, 10])\n","torch.Size([50])\n","torch.Size([2, 50])\n","torch.Size([2])\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"o1sr5NNI19r3","colab_type":"text"},"source":["Notice, that for each layer, say the first layer, we have a weight matrix of shape (50 x 10) and a bias vector of shape 50. Likewise for the second layer.\n","Let's try to pass some inputs to the model and check the outputs. While defining the model, we need not care about the batch (or rather minibatch) size of the input. The first dimension of the input is always considered to be the batch dimension."]},{"cell_type":"code","metadata":{"id":"Yw67_8jO19r7","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"b87faf66-a497-4749-a001-1828ebd32ba4","executionInfo":{"status":"ok","timestamp":1583096600853,"user_tz":-330,"elapsed":1831,"user":{"displayName":"Rahul Bishain","photoUrl":"","userId":"06667514033618368895"}}},"source":["batch_size = 32\n","input = torch.rand(batch_size, input_dim)\n","output = model(input)\n","\n","# Let's check the size of the output\n","print(output.size())"],"execution_count":3,"outputs":[{"output_type":"stream","text":["torch.Size([32, 2])\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"W16w7Z_F19sB","colab_type":"text"},"source":["## **Intro to nn.Module of PyTorch**"]},{"cell_type":"markdown","metadata":{"id":"mUF5BBjW19sE","colab_type":"text"},"source":["In this module we'll introduce you to PyTorch's nn.Module class, and how you can use them to introduce complicated architectures. <br>\n","\n","It is not possible to create all forms of architecture using nn.Sequential, many architectures such as ResNet and Inception do not have a linear sequence in which data is passed through. <br>\n","\n","For example following is a ResNet block, which can not be implemented using nn.Sequential.\n","![image](resnet_block.png)\n","\n","However its easy to implement such blocks using nn.Module. <br>\n","\n","In this class, we'll first introduce how to form a simple fully connected architecture and CNN using nn.Module.\n","\n","Then we'll use the power of nn.Module to show how more complex architectrues can be formed, like the ResNet block above."]},{"cell_type":"code","metadata":{"id":"st657Ip319sF","colab_type":"code","colab":{}},"source":["## Let's start by constructing a fully connected nn.Module, equivalent to one we constructed in previous class using\n","## nn.Sequential\n","\n","## When defining a model using nn.Moudule two methods need to implemented i.e __init__ and forward.\n","## nn.Module takes care of the backward pass for you.\n","class TwoLayerNet(nn.Module):\n","    def __init__(self, D_in, H, D_out):\n","        '''\n","        D_in: Dimensionality of the input\n","        H: Hidden layer dimensionality\n","        D_out: Dimensionality of the output\n","        '''\n","        super(TwoLayerNet, self).__init__()\n","        \n","        # Define all layers which have weights here.\n","        self.linear1 = nn.Linear(D_in, H)  \n","        self.linear2 = nn.Linear(H, D_out)\n","        self.relu = nn.ReLU()\n","        \n","    def forward(self, x):\n","        '''\n","        x: input tensor of dimensionality (b, D_in), where b is batch size and D_in is input dimension.\n","        \n","        returns: y_pred, of shape (b, D_out), where D_out is output dimension.\n","        '''\n","        h_relu = self.relu(self.linear1(x)) # nn.Module layers can be called as a function on the input\n","        \n","        # Using for displaying the shape of hidden layer\n","        # DON'T WRITE THIS when actually implementing your models\n","        print(\"\\nHidden Activation Layer has shape: {}\".format(h_relu.shape)) \n","        \n","        y_pred = self.linear2(h_relu) \n","        return y_pred"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"0Fjt1CSg19sN","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":208},"outputId":"e9d39f05-2de7-494d-ad18-a0069fb4a38f","executionInfo":{"status":"ok","timestamp":1583096600860,"user_tz":-330,"elapsed":1828,"user":{"displayName":"Rahul Bishain","photoUrl":"","userId":"06667514033618368895"}}},"source":["## In the block above we defined a simple 2-layer fully connected network, now let's pass an input to it.\n","\n","# N is batch size; D_in is input dimension;\n","# H is the dimension of the hidden layer; D_out is output dimension.\n","N, D_in, H, D_out = 32, 100, 50, 10\n","\n","# Construct our model by instantiating the class defined above\n","model = TwoLayerNet(D_in, H, D_out)\n","\n","# Lets check the shapes of each layer\n","print(model)\n","\n","\n","# Create random Tensors to hold inputs and outputs, and wrap them in Variables\n","x = torch.randn(N, D_in)  # dim: 32 x 100\n","\n","# Lets check shape of x and its type\n","print(\"\\nx has shape {}\".format(x.shape))\n","\n","\n","\n","\n","# Forward pass: Compute predicted y by passing x to the model\n","# nn.Module model can be used on input by directly calling the model on x.\n","y_pred = model(x)   # dim: 32 x 10\n","\n","# Lets check the shape of each y , the output\n","\n","print(\"\\ny_pred has shape: {}\".format(y_pred.shape))"],"execution_count":5,"outputs":[{"output_type":"stream","text":["TwoLayerNet(\n","  (linear1): Linear(in_features=100, out_features=50, bias=True)\n","  (linear2): Linear(in_features=50, out_features=10, bias=True)\n","  (relu): ReLU()\n",")\n","\n","x has shape torch.Size([32, 100])\n","\n","Hidden Activation Layer has shape: torch.Size([32, 50])\n","\n","y_pred has shape: torch.Size([32, 10])\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"K2aejZ4Z19sS","colab_type":"text"},"source":["Notice how easy it is to construct a network using nn.Module, we only need to define how the forward pass of our function looks like, and we can use the data from intermediate layers as well. Next lets' construct a Basic Convolutional Network"]},{"cell_type":"markdown","metadata":{"id":"93RF8mCz19sT","colab_type":"text"},"source":["## Basic Convolutional Network using nn.Module"]},{"cell_type":"markdown","metadata":{"id":"W9WW5Vru19sV","colab_type":"text"},"source":["Now that we know how to implement a simple neural network using nn.Module we'll next construct a basic convnet using nn.Module and also show how nn.Module takes care of the back prop on its own."]},{"cell_type":"code","metadata":{"id":"hSsEcX6f19sX","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":173},"outputId":"0bdc7321-7946-4c5c-edb2-1ffaa8f954ae","executionInfo":{"status":"ok","timestamp":1583096600863,"user_tz":-330,"elapsed":1825,"user":{"displayName":"Rahul Bishain","photoUrl":"","userId":"06667514033618368895"}}},"source":["class BasicConvNet(nn.Module):\n","\n","    def __init__(self):\n","        super(BasicConvNet, self).__init__()\n","        \n","       \n","        # Again define all layers with weights here.\n","        \n","        self.conv1 = nn.Conv2d(1, 6, 5) # 1 input image channel, 6 output channels, 5x5 square convolution kernel\n","        self.conv2 = nn.Conv2d(6, 16, 5)\n","        # an affine operation: y = Wx + b\n","        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n","        self.fc2 = nn.Linear(120, 84)\n","        self.fc3 = nn.Linear(84, 10)\n","        self.relu = nn.ReLU()\n","        self.max_pool2d = nn.MaxPool2d(2)\n","\n","    def forward(self, x):\n","        \n","        # Input-> Conv1 -> ReLU\n","        x = self.relu(self.conv1(x))\n","        print(\"Conv1 Activation has output size: {}\\n\".format(x.shape))\n","        \n","        # Max pooling over a (2, 2) window\n","        x = self.max_pool2d(x)  \n","        print(\"MaxPool1 has output size: {}\\n\".format(x.shape))\n","        \n","        # If the size is a square you can only specify a single number\n","        x = self.relu(self.conv2(x))\n","        print(\"Conv2 Activation has output size: {}\\n\".format(x.shape))\n","        \n","        x = self.max_pool2d(x)\n","        print(\"MaxPool2 has output size: {}\\n\".format(x.shape))\n","        \n","        # x.view is used to flatten the output. if -1 is used, the given dimension is inferred.\n","        \n","        x = x.view(x.size(0), -1) \n","        print(\"Flattened output has shape: {}\\n\".format(x.shape))\n","        \n","        x = self.relu(self.fc1(x))\n","        print(\"Linear1 Activation has shape: {}\\n\".format(x.shape))\n","        \n","        x = self.relu(self.fc2(x))\n","        print(\"Linear2 Activation has shape: {}\\n\".format(x.shape))\n","        \n","        x = self.fc3(x)\n","        return x\n","\n","\n","net = BasicConvNet()\n","print(net)"],"execution_count":6,"outputs":[{"output_type":"stream","text":["BasicConvNet(\n","  (conv1): Conv2d(1, 6, kernel_size=(5, 5), stride=(1, 1))\n","  (conv2): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))\n","  (fc1): Linear(in_features=400, out_features=120, bias=True)\n","  (fc2): Linear(in_features=120, out_features=84, bias=True)\n","  (fc3): Linear(in_features=84, out_features=10, bias=True)\n","  (relu): ReLU()\n","  (max_pool2d): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",")\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"VU63WCMd19sf","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"1c001a5a-3448-440e-8342-35057c7ba435","executionInfo":{"status":"ok","timestamp":1583096601356,"user_tz":-330,"elapsed":2313,"user":{"displayName":"Rahul Bishain","photoUrl":"","userId":"06667514033618368895"}}},"source":["params = list(net.parameters()) # Retrieve the parameters of net.\n","print(\"Conv1's weights: {}\".format(params[0].size()))  # conv1's .weight"],"execution_count":7,"outputs":[{"output_type":"stream","text":["Conv1's weights: torch.Size([6, 1, 5, 5])\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"44-R4xnR19sj","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":364},"outputId":"d19c9d91-1303-4b4c-f4f0-2cc11099fdb1","executionInfo":{"status":"ok","timestamp":1583096601357,"user_tz":-330,"elapsed":2309,"user":{"displayName":"Rahul Bishain","photoUrl":"","userId":"06667514033618368895"}}},"source":["input = torch.randn(1, 1, 32, 32) # Input shape is (batch_size, number_of_channels, height, width)\n","out = net(input) # Get the output\n","\n","print(\"\\nShape of output: {}\\n\".format(out.shape))\n","print(\"Output is : \\n{}\".format(out)) "],"execution_count":8,"outputs":[{"output_type":"stream","text":["Conv1 Activation has output size: torch.Size([1, 6, 28, 28])\n","\n","MaxPool1 has output size: torch.Size([1, 6, 14, 14])\n","\n","Conv2 Activation has output size: torch.Size([1, 16, 10, 10])\n","\n","MaxPool2 has output size: torch.Size([1, 16, 5, 5])\n","\n","Flattened output has shape: torch.Size([1, 400])\n","\n","Linear1 Activation has shape: torch.Size([1, 120])\n","\n","Linear2 Activation has shape: torch.Size([1, 84])\n","\n","\n","Shape of output: torch.Size([1, 10])\n","\n","Output is : \n","tensor([[-0.0241,  0.0893, -0.0318,  0.0329,  0.0392, -0.0038, -0.0329, -0.0719,\n","         -0.0481,  0.0742]], grad_fn=<AddmmBackward>)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"XV9V9gs-19so","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":104},"outputId":"d514bafa-b880-46ed-ef7a-33e5efa190f6","executionInfo":{"status":"ok","timestamp":1583096604059,"user_tz":-330,"elapsed":5005,"user":{"displayName":"Rahul Bishain","photoUrl":"","userId":"06667514033618368895"}}},"source":["# Lets now see how the backward pass of nn.Module can be computed directly.\n","\n","net.zero_grad()     # zeroes the gradient buffers of all parameters\n","\n","print('conv1.bias.grad before backward')\n","print(net.conv1.bias.grad)\n","\n","out.backward(torch.randn(1, 10)) # Pass a random gradient in the buffer\n","\n","print('conv1.bias.grad after backward')\n","print(net.conv1.bias.grad)\n","!pwd"],"execution_count":9,"outputs":[{"output_type":"stream","text":["conv1.bias.grad before backward\n","None\n","conv1.bias.grad after backward\n","tensor([-0.0455, -0.0152,  0.0326,  0.0486, -0.0575, -0.0077])\n","/content\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"LLdioIS819sv","colab_type":"text"},"source":["## ResNet Block\n","\n","Now we have seen how to construct basic architecture's using nn.Module, we will next see modules which cannot be constructed using nn.Sequential, as the data flow within these architectures is not linear, we can re-use our intermediate layers directly. We will implement ResNet Block using nn.Module now as shown below.\n","<br>\n","![ResNet Block](https://miro.medium.com/max/1140/1*D0F3UitQ2l5Q0Ak-tjEdJg.png)\n","\n","\n","In a single resnet block a skip connection is present between the input layers and output layers of the block."]},{"cell_type":"code","metadata":{"id":"Mn0G6wFp19sw","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":104},"outputId":"4f99aa6e-262a-4989-ffbb-af2c21fd08d9","executionInfo":{"status":"ok","timestamp":1583096604060,"user_tz":-330,"elapsed":4999,"user":{"displayName":"Rahul Bishain","photoUrl":"","userId":"06667514033618368895"}}},"source":["class BasicBlock(nn.Module):\n","\n","    def __init__(self, in_planes, out_planes, stride=1, downsample=None):\n","        '''\n","        in_planes: number of input channels\n","        out_planes: number of output channels\n","        stride: stride to set for conv layers\n","        downsample: function used to downsample before skip-connection \n","        '''\n","        super(BasicBlock, self).__init__()\n","        \n","        self.conv1 = nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride,\n","                     padding=1, bias=False)\n","        \n","        \n","        self.relu = nn.ReLU(inplace=True)\n","        \n","        self.conv2 = nn.Conv2d(out_planes, out_planes, kernel_size=3, stride=stride,\n","                     padding=1, bias=False)\n","                               \n","        self.stride = stride\n","\n","    def forward(self, x):\n","        identity = x\n","\n","        out = self.conv1(x)\n","        out = self.relu(out)\n","        out = self.conv2(out)\n","               \n","        # Notice the skip connection!! such a connection isn't possible with nn.Sequential.\n","        out += identity \n","        out = self.relu(out)\n","\n","        return out\n","\n","    \n","block = BasicBlock(2, 2)\n","print(block)"],"execution_count":10,"outputs":[{"output_type":"stream","text":["BasicBlock(\n","  (conv1): Conv2d(2, 2, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","  (relu): ReLU(inplace=True)\n","  (conv2): Conv2d(2, 2, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",")\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"VfJkhhvR19s1","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":52},"outputId":"2bdbb191-68e2-4610-c9d4-92933857fdef","executionInfo":{"status":"ok","timestamp":1583096604060,"user_tz":-330,"elapsed":4994,"user":{"displayName":"Rahul Bishain","photoUrl":"","userId":"06667514033618368895"}}},"source":["input = torch.randn(1, 2, 5, 5)\n","\n","output = block(input)\n","\n","\n","print(\"Shape of the output layer: {}\\n\".format(output.shape)) "],"execution_count":11,"outputs":[{"output_type":"stream","text":["Shape of the output layer: torch.Size([1, 2, 5, 5])\n","\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"jQZo7kKA19s5","colab_type":"text"},"source":["Here we have shown how to construct complex architectures using nn.Module, which allows you to access the internal activations of the network and pass them onto other layers. We displayed this functionality ResNet block. <br>\n","As you become more advanced with PyTorch"]}]}